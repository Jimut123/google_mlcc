{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLCC-week1-task.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"G4-r3qKgPa2t","colab_type":"toc"},"cell_type":"markdown","source":[">[MLCC Week 1 Task](#scrollTo=toNrmbxxHLOL)\n","\n",">[Step 1: Getting the data](#scrollTo=PikbkmYRH_Ls)\n","\n",">[Step 2: Cleaning the data](#scrollTo=Yq6B0BseIhHj)\n","\n",">[Step 3: Configuring the LinearRegressor](#scrollTo=-deOxlscJ_ud)\n","\n",">[Task 1: Get the loss less than 15000](#scrollTo=1ce2H07bKKxm)\n","\n",">[Task 2: Take another feature](#scrollTo=inRwqESaLqiE)\n","\n"]},{"metadata":{"id":"toNrmbxxHLOL","colab_type":"text"},"cell_type":"markdown","source":["# MLCC Week 1 Task"]},{"metadata":{"id":"HyYgFOJAT7VU","colab_type":"text"},"cell_type":"markdown","source":["Online news websites play an important role in our day to day life. Not only we read news online, we also share it as per our liking.\n","\n","Just like the cover of a book makes the first impression, the title of the news can have a lasting effect on the popularity of the news\n","\n","In this task, we will be using a database of online news popularity provided by UCL. Your task is to predict how many **shares** an online news will get depending on the **number of tokens** (roughly speaking, tokens are words that are important) in its title."]},{"metadata":{"id":"PikbkmYRH_Ls","colab_type":"text"},"cell_type":"markdown","source":["# Step 1: Getting the data"]},{"metadata":{"id":"Dtz9jPZXIFJS","colab_type":"text"},"cell_type":"markdown","source":["The file provided by UCL is a zip file. So we will use Python magic to unzip and take the csv\n","\n","We use the [wget module](https://pypi.org/project/wget/) to download the file and [zipfile module](https://docs.python.org/3/library/zipfile.html) to unzip"]},{"metadata":{"id":"JhZts9lDL14w","colab_type":"code","colab":{}},"cell_type":"code","source":["! pip install wget # The ! before suggests it's a shell command. We install wget module to ease the download process\n","import wget\n","filename = wget.download('https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip') # filename stores the name of the downloaded file"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FH1_94RhMG-t","colab_type":"code","colab":{}},"cell_type":"code","source":["print(filename)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EF4Jp3KUVKhy","colab_type":"text"},"cell_type":"markdown","source":["Now we use the zipfile module provided by Python to unzip the file in current directory."]},{"metadata":{"id":"DU5MIDJNMIjZ","colab_type":"code","colab":{}},"cell_type":"code","source":["import zipfile\n","zip_ref = zipfile.ZipFile(filename, 'r')\n","zip_ref.extractall('.')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K_bXOrD_VUn0","colab_type":"text"},"cell_type":"markdown","source":["Let's list all the files in the extracted directory. Make sure there is one csv and one names file"]},{"metadata":{"id":"2urUtzmRLIaO","colab_type":"code","colab":{}},"cell_type":"code","source":["! ls OnlineNewsPopularity/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yq6B0BseIhHj","colab_type":"text"},"cell_type":"markdown","source":["# Step 2: Cleaning the data"]},{"metadata":{"id":"G8SEsHodVeLE","colab_type":"text"},"cell_type":"markdown","source":["Now we move on to our usual routine. First, all the imports"]},{"metadata":{"id":"dPwqXx-SMxFX","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","\n","from IPython import display\n","from matplotlib import cm\n","from matplotlib import gridspec\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","import tensorflow as tf\n","from tensorflow.python.data import Dataset\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = '{:.1f}'.format"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LtKYgFXQVkY_","colab_type":"text"},"cell_type":"markdown","source":["Now we read the file in the dataframe."]},{"metadata":{"id":"viB8PhQ_M14j","colab_type":"code","colab":{}},"cell_type":"code","source":["popularity_dataframe = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n","popularity_dataframe.describe()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ay3fwm82VrZX","colab_type":"text"},"cell_type":"markdown","source":["Don't foreget to randomize the data. \n","\n","For simplicity and faster result, we drop any rows which have more than 500 shares\n","\n","We also scale the \"shares\" column by 100 so that's they're spaced (somewhat) apart and are easy to visualized. This makes our data in the range of 50000. So we should expect our RMSE to be measured in the same scale.\n","\n","**N.B.** The column names have an empty space before them. Tensorflow doesn't like having a space in its scope name. So we change the name of the column"]},{"metadata":{"id":"4TCcMTCINUNH","colab_type":"code","colab":{}},"cell_type":"code","source":["popularity_dataframe.reindex(np.random.permutation(popularity_dataframe.index))\n","_ = popularity_dataframe[' shares'] # Look closely! There's a space in the beginning\n","popularity_dataframe = popularity_dataframe.drop(popularity_dataframe[popularity_dataframe[' shares'] > 500].index) # Drop the rows with more than 500 shares\n","popularity_dataframe['normalized_shares'] = popularity_dataframe[' shares'] * 100 # Scale up by 100\n","popularity_dataframe.rename(columns={' n_tokens_title': 'n_tokens_title'}, inplace=True) # Rename the column and remove the space\n","popularity_dataframe.describe()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A7fk3JlfWZzW","colab_type":"text"},"cell_type":"markdown","source":["Our feature column would be the **n_tokens_title** column and target would be the **normalized_shares column**. Let's plot these two."]},{"metadata":{"id":"AqZVyOaMNhO_","colab_type":"code","colab":{}},"cell_type":"code","source":["X = popularity_dataframe[['n_tokens_title']]\n","Y = popularity_dataframe['normalized_shares']\n","plt.scatter(X, Y)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-deOxlscJ_ud","colab_type":"text"},"cell_type":"markdown","source":["# Step 3: Configuring the LinearRegressor"]},{"metadata":{"id":"TbsannP_XGlF","colab_type":"text"},"cell_type":"markdown","source":["Now as usual, we create the feature columns and create the LinearRegressor.\n","\n","We start by defining the input function which we have used before"]},{"metadata":{"id":"xiUIt6hCPW67","colab_type":"code","colab":{}},"cell_type":"code","source":["def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n","    \"\"\"Trains a linear regression model of one feature.\n","    Args:\n","      features: pandas DataFrame of features\n","      targets: pandas DataFrame of targets\n","      batch_size: Size of batches to be passed to the model\n","      shuffle: True or False. Whether to shuffle the data.\n","      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n","    Returns:\n","      Tuple of (features, labels) for next data batch\n","    \"\"\"\n","    \n","    # Convert pandas data into a dict of np arrays.\n","    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n","\n","    # Construct a dataset, and configure batching/repeating.\n","    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n","    ds = ds.batch(batch_size).repeat(num_epochs)\n","\n","    # Shuffle the data, if specified.\n","    if shuffle:\n","      ds = ds.shuffle(buffer_size=10000)\n","      \n","    # Return the next batch of data.\n","    features, labels = ds.make_one_shot_iterator().get_next()\n","\n","    return features, labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xzeoYtHpZc_E","colab_type":"text"},"cell_type":"markdown","source":["The rest of the routine jobs of configuring the regressor, training and predicting have been wrapped in this convenient function which can be called with the parameters learning rate, steps, batch size and feature column"]},{"metadata":{"id":"pekzUCATY2jg","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_model(learning_rate, steps, batch_size, input_feature=\"n_tokens_title\"):\n","  \"\"\"Trains a linear regression model of one feature.\n","  \n","  Args:\n","    learning_rate: A `float`, the learning rate.\n","    steps: A non-zero `int`, the total number of training steps. A training step\n","      consists of a forward and backward pass using a single batch.\n","    batch_size: A non-zero `int`, the batch size.\n","    input_feature: A `string` specifying a column from `popularit_dataframe`\n","      to use as input feature.\n","  \"\"\"\n","  \n","  periods = 10\n","  steps_per_period = steps / periods\n","\n","  my_feature = input_feature\n","  my_feature_data = popularity_dataframe[[my_feature]]\n","  my_label = \"normalized_shares\"\n","  targets = popularity_dataframe[my_label]\n","\n","  # Create feature columns.\n","  feature_columns = [tf.feature_column.numeric_column(my_feature)]\n","  \n","  # Create input functions.\n","  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)\n","  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n","  \n","  # Create a linear regressor object.\n","  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n","  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n","  linear_regressor = tf.estimator.LinearRegressor(\n","      feature_columns=feature_columns,\n","      optimizer=my_optimizer\n","  )\n","\n","  # Set up to plot the state of our model's line each period.\n","  plt.figure(figsize=(15, 6))\n","  plt.subplot(1, 2, 1)\n","  plt.title(\"Learned Line by Period\")\n","  plt.ylabel(my_label)\n","  plt.xlabel(my_feature)\n","  sample = popularity_dataframe.sample(n=300)\n","  plt.scatter(sample[my_feature], sample[my_label])\n","  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n","\n","  # Train the model, but do so inside a loop so that we can periodically assess\n","  # loss metrics.\n","  print(\"Training model...\")\n","  print(\"RMSE (on training data):\")\n","  root_mean_squared_errors = []\n","  for period in range (0, periods):\n","    # Train the model, starting from the prior state.\n","    linear_regressor.train(\n","        input_fn=training_input_fn,\n","        steps=steps_per_period\n","    )\n","    # Take a break and compute predictions.\n","    predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n","    predictions = np.array([item['predictions'][0] for item in predictions])\n","    \n","    # Compute loss.\n","    root_mean_squared_error = math.sqrt(\n","        metrics.mean_squared_error(predictions, targets))\n","    # Occasionally print the current loss.\n","    print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n","    # Add the loss metrics from this period to our list.\n","    root_mean_squared_errors.append(root_mean_squared_error)\n","    # Finally, track the weights and biases over time.\n","    # Apply some math to ensure that the data and line are plotted neatly.\n","    y_extents = np.array([0, sample[my_label].max()])\n","    \n","    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n","    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n","\n","    x_extents = (y_extents - bias) / weight\n","    x_extents = np.maximum(np.minimum(x_extents,\n","                                      sample[my_feature].max()),\n","                           sample[my_feature].min())\n","    y_extents = weight * x_extents + bias\n","    plt.plot(x_extents, y_extents, color=colors[period]) \n","  print(\"Model training finished.\")\n","\n","  # Output a graph of loss metrics over periods.\n","  plt.subplot(1, 2, 2)\n","  plt.ylabel('RMSE')\n","  plt.xlabel('Periods')\n","  plt.title(\"Root Mean Squared Error vs. Periods\")\n","  plt.tight_layout()\n","  plt.plot(root_mean_squared_errors)\n","\n","  # Output a table with calibration data.\n","  calibration_data = pd.DataFrame()\n","  calibration_data[\"predictions\"] = pd.Series(predictions)\n","  calibration_data[\"targets\"] = pd.Series(targets)\n","  display.display(calibration_data.describe())\n","\n","  print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MzsLoBDIKFIs","colab_type":"text"},"cell_type":"markdown","source":["Now we can call the function"]},{"metadata":{"id":"SD7B3EvFZLDO","colab_type":"code","colab":{},"cellView":"both"},"cell_type":"code","source":["train_model(\n","    learning_rate=0.1,\n","    steps=100,\n","    batch_size=1\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1ce2H07bKKxm","colab_type":"text"},"cell_type":"markdown","source":["# Task 1: Get the loss less than 15000"]},{"metadata":{"id":"OZYpa8ixKQQi","colab_type":"text"},"cell_type":"markdown","source":["Tweak the learning rate until you have a loss less than 15000\n","**Hint**:\n","\n","\n","1.   Check the initial few lines. If they are too close, increase the learning rate. If they are too sparse, decrease it.\n","2.   The loss function should decrease steeply at first, and then decrease slowly.\n","3. If the training has not converged, try running it for longer. Change the **steps** parameter for that\n","4. If the loss function first decreases and then increases, it means you have overshot the minima. Try decreasing the learning rate.\n","5. Very small batch size can cause problem, and so can very large batch size.\n","\n"]},{"metadata":{"id":"inRwqESaLqiE","colab_type":"text"},"cell_type":"markdown","source":["# Task 2: Take another feature"]},{"metadata":{"id":"tmaJhxowLujS","colab_type":"text"},"cell_type":"markdown","source":["Take any other feature and see if you can do better or not.\n","\n","Remember, there might be spaces in front of the column name. Make sure to rename the columns, if needed"]},{"metadata":{"id":"TAyJ4vQzL1jL","colab_type":"code","colab":{}},"cell_type":"code","source":["# Rename the columns if necessary\n","# popularity_dataframe.rename(...) \n","train_model(\n","    learning_rate=0.1,\n","    steps=100,\n","    batch_size=1,\n","    input_feature = # YOUR CODE HERE\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2q3MjD7dONl2","colab_type":"text"},"cell_type":"markdown","source":["Created by Aniket Bhattacharyea (aniketmail669@gmail.com)"]}]}